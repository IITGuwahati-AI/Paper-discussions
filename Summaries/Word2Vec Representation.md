


## **Word2Vec Representation Model**

> **Quick Overview**
     

 1. The main goal of this paper is to introduce techniques that can be
   used for learning high-quality word vectors from huge data sets with
   billions of words, and with millions of words in the vocabulary.​
       
 2. It proposes two novel model architectures - CBOW and Skip-Gram ,   for computing continuous vector representations of words from very   large data sets.​

   
 
> [**Presentation made for the discussion**](https://drive.google.com/file/d/1Hwi-Iy1tgr-N3zHoRFh0pLE5PN6cuk3s/view?usp=sharing)


> **Resources**
> 

 1. [Paper](https://arxiv.org/abs/1301.3781)
 2. [Video](https://www.youtube.com/watch?v=ERibwqs9p38)

