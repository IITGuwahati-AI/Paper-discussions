


## **Word2Vec Representation Model**

> **Quick Overview**
     

 1. The main goal of this paper is to introduce techniques that can be
   used for learning high-quality word vectors from huge data sets with
   billions of words, and with millions of words in the vocabulary.​
       
 2. It proposes two novel model architectures - CBOW and Skip-Gram ,   for computing continuous vector representations of words from very   large data sets.​

   
 
> [**Presentation made for the discussion**](https://iitgoffice-my.sharepoint.com/:p:/g/personal/vbakshi_iitg_ac_in/EZv1asVGWlJLuLarYn2EZKIBkd6WXCfHXhUp9p0wgF2y_Q?e=xFcrGe)


> **Resources**
> 

 1. [Paper](https://arxiv.org/abs/1301.3781)
 2. [Video](https://www.youtube.com/watch?v=ERibwqs9p38)

